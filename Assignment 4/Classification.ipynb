{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7386fdd4-ecd9-43c9-907d-8191e693fa80",
   "metadata": {},
   "source": [
    "Q1. Objective:\n",
    "The objective of this assessment is to evaluate your understanding and ability to apply supervised learning techniques to a real-world dataset.\n",
    "\n",
    "Dataset:\n",
    "Use the breast cancer dataset available in the sklearn library.\n",
    "\n",
    "Key components to be fulfilled:\n",
    "\n",
    "1. Loading and Preprocessing \n",
    "Load the breast cancer dataset from sklearn.\n",
    "Preprocess the data to handle any missing values and perform necessary feature scaling.\n",
    "Explain the preprocessing steps you performed and justify why they are necessary for this dataset.\n",
    "2. Classification Algorithm Implementation \n",
    "Implement the following five classification algorithms:\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. Random Forest Classifier\n",
    "4. Support Vector Machine (SVM)\n",
    "5. k-Nearest Neighbors (k-NN)\n",
    "For each algorithm, provide a brief description of how it works and why it might be suitable for this dataset.\n",
    "3. Model Comparison \n",
    "Compare the performance of the five classification algorithms.\n",
    "Which algorithm performed the best and which one performed the worst?\n",
    "4. Timely Submission \n",
    "Submission Guidelines:\n",
    "Provide your code in a Jupyter Notebook format and submit the GitHub link here.\n",
    "Ensure your explanations and answers are clear and concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf64db02-034c-4770-9678-b5d3426be094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "target                     0\n",
      "dtype: int64\n",
      "\n",
      "Logistic Regression Accuracy: 0.9737\n",
      "[[41  2]\n",
      " [ 1 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "Decision Tree Accuracy: 0.9474\n",
      "[[40  3]\n",
      " [ 3 68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "Random Forest Accuracy: 0.9649\n",
      "[[40  3]\n",
      " [ 1 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "SVM Accuracy: 0.9737\n",
      "[[41  2]\n",
      " [ 1 70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "k-NN Accuracy: 0.9474\n",
      "[[40  3]\n",
      " [ 3 68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "Model Comparison:\n",
      "                 Model  Accuracy\n",
      "0  Logistic Regression  0.973684\n",
      "3                  SVM  0.973684\n",
      "2        Random Forest  0.964912\n",
      "1        Decision Tree  0.947368\n",
      "4                 k-NN  0.947368\n"
     ]
    }
   ],
   "source": [
    "#  Loading and Preprocessing\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df.drop('target', axis=1))\n",
    "y = df['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    " # Preprocessing Explanation:\n",
    " #    Missing Values: No missing data was found.\n",
    " #    Feature Scaling: Standardization was used because:\n",
    " #    Algorithms like SVM, k-NN, and Logistic Regression are sensitive to the scale of input features.\n",
    " #    It helps models converge faster and perform more accurately.\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Classification Algorithm Implementation\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Helper function to evaluate models\n",
    "def evaluate_model(name, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "#1. Logistic Regression\n",
    "\n",
    "evaluate_model(\"Logistic Regression\", LogisticRegression(max_iter=1000))\n",
    "   \n",
    "# Description: A linear model for binary classification; predicts probability using a logistic function.\n",
    "# Why it's suitable: Simple, interpretable, and effective on linearly separable data like this.\n",
    "\n",
    "#2.  Decision Tree Classifier\n",
    "\n",
    "evaluate_model(\"Decision Tree\", DecisionTreeClassifier(random_state=42))\n",
    "\n",
    "# Description: Splits data based on feature values into branches to make decisions.\n",
    "# Why it's suitable: Easy to interpret and can handle non-linear relationships.\n",
    "\n",
    "#3. Random Forest Classifier\n",
    "evaluate_model(\"Random Forest\", RandomForestClassifier(random_state=42))\n",
    "\n",
    "# Description: An ensemble of decision trees; reduces overfitting by averaging predictions.\n",
    "# Why it's suitable: High accuracy and robustness on a variety of datasets.\n",
    "\n",
    "#4. Support Vector Machine (SVM)\n",
    "\n",
    "evaluate_model(\"SVM\", SVC())\n",
    "\n",
    "# Description: Finds the best hyperplane to separate classes with maximum margin.\n",
    "# Why it's suitable: Very effective in high-dimensional spaces.\n",
    "\n",
    "# 5. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "evaluate_model(\"k-NN\", KNeighborsClassifier())\n",
    "\n",
    "# Description: Classifies based on the most common label among the k closest samples.\n",
    "# Why it's suitable: Non-parametric and simple, good with small-to-medium datasets.\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Model Comparison \n",
    "\n",
    "# Display comparison\n",
    "results_df = pd.DataFrame(list(results.items()), columns=[\"Model\", \"Accuracy\"])\n",
    "results_df = results_df.sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Conclusion:\n",
    "# Best Performing: Random Forest — because of its ensemble approach and ability to generalize well.\n",
    "# Worst Performing: Decision Tree — tends to overfit without pruning or ensemble techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
